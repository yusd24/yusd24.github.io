---
title: Singular value theorem
output:
  md_document:
    variant: gfm+footnotes
    preserve_yaml: TRUE
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../_posts") })
date: 2025-05-01
permalink: /posts/2025-svd
excerpt_separator: <!--more-->
always_allow_html: true
toc: true
header:
 og_image: "posts/nest-map/fig-1.png"
tags:
  - SVD
  - linear algebra
---

Prove

<!--more-->

## Singular value decomposition (SVD) for linear transformations

Let \\(T\colon V\to V\\) be a linear operator
on a finite-dimensional vector space \\(V\\) over \\(F=\mathbf{R}\\) or \\(\mathbf{C}\\).
We have seen that 
\begin{itemize}
  \item for \(F=\mathbf{C}\), if \(T\) is normal, then there
  exists an orthonormal basis for \(V\) consisting of eigenvectors of 
  \(T\);
  \item for \(F=\mathbf{R}\), if \(T\) is self-adjoint, then there
  exists an orthonormal basis for \(V\) consisting of eigenvectors of 
  \(T\).
\end{itemize}
We can interprete the above statements as follows. The eigenvalues
of \(T\) measure how much the eigenvectors in the corresponding 
orthonormal basis are distorted, that is,
\begin{equation*}
  T(v)=\lambda v.
\end{equation*}

Our goal is to generalize the above statements to general linear maps 
\(T\colon V\to W\) between two possibly different inner 
product spaces. Of course, in this situation, it does not 
make sense to talk about eigenvalues and eigenvectors. But still,
we can measure how much the vectors in two different orthonormal bases
(one for \(V\) and another for \(W\)) are distorted.

For this purpose, we need to search for nice orthonormal bases \(\beta\)
for \(V\) and \(\gamma\) for \(W\), and, in the meantime, hoping that 
\(T\) takes vectors in \(\beta\) to vectors in \(\gamma\). Suppose
\(\beta=\{v_{1},\ldots,v_{n}\}\) and \(\gamma=\{w_{1},\ldots,w_{m}\}\).
At the very least, we would like to demand that 
\begin{equation*}
  \langle T(v_{i}),T(w_{j})\rangle_{W}=\delta_{ij}.
\end{equation*}
In other words,
\begin{equation*}
  \langle T^{\ast}T(v_{i}),v_{j}\rangle_{V}=\delta_{ij}.
\end{equation*}
Note that \(T^{\ast}T\) is self-adjoint and \emph{positive semidefinite}. We 
can thus simply pick \(\beta\) to be the orthonormal basis for \(V\)
consisting of eigenvectors of \(T^{\ast}T\). Write \(\beta=\{v_{1},\ldots,v_{n}\}\).
We can arrange \(v_{i}\) according to the magnitude of the corresponding eigenvalue \(\lambda_{i}\);
that is, we assume that 
\begin{equation}
  \lambda_{1}\ge\lambda_{2}\ge\cdots \ge\lambda_{n-1}\ge\lambda_{n}.
\end{equation}
Now let \(r=\mathrm{rank}(T)\). Then \(r=\mathrm{rank}(T^{\ast}T)\) and 
\begin{equation}
  \lambda_{1}\ge\lambda_{2}\ge\cdots \ge\lambda_{r}>\lambda_{r+1}=\cdots=\lambda_{n}=0.
\end{equation}
This implies that \(T(v_{i})\) is non-zero for \(1\le i\le r\). Moreover,
\begin{equation}
    \langle T(v_{i}),T(w_{j})\rangle_{W}=
    \langle T^{\ast}T(v_{i}),v_{j}\rangle_{V}=\lambda_{i}\delta_{ij}.
\end{equation}
If we set \(w_{i}:=T(v_{i})\slash\sqrt{\lambda_{i}}\) for \(1\le i\le r\), then 
\begin{equation}
  \langle w_{i},w_{j}\rangle_{W}=\delta_{ij}~\mbox{for}~1\le i,j\le r.
\end{equation}
Said differently, the ste \(\{w_{1},\ldots,w_{r}\}\) is an orthonormal subset of \(W\),
and we can extend it to an orthonormal basis 
\begin{equation*}
  \gamma=\{w_{1},\ldots,w_{r},w_{r+1},\ldots,w_{m}\}
\end{equation*}
for \(W\). We then arrive at the following theorem.
\begin{theorem}[The singular value theorem for linear maps]
Let \(V\) and \(W\) be finite-dimensional inner product spaces 
and \(T\colon V\to W\) be a linear map of rank \(r\).
Then there exist an orthonormal basis \(\beta=\{v_{1},\ldots,v_{n}\}\) for \(V\),
an orthonormal basis \(\gamma=\{w_{1},\ldots,w_{m}\}\) for \(W\), and
positive scalars \(\sigma_{1}\ge\cdots\ge\sigma_{r}>0\) such that
\begin{equation*}
  T(v_{i})=\begin{cases}
  \sigma_{i}w_{i},~&\mbox{for}~1\le i\le r\\
  \mathbf{0},~&\mbox{otherwise}.
  \end{cases}
\end{equation*}
Conversely, suppose the previous conditions are satisfied. Then for 
\(1\le i\le n\) the vector \(v_{i}\) is an eigenvector of \(T^{\ast}T\)
wit corresponding eigenvalue \(\sigma_{i}^{2}\) for \(1\le i\le r\), and \(0\)
for \(r+1\le i\le n\). Therefore, the scalars \(\sigma_{i}\)'s are uniquely determined by \(T\).
\end{theorem}
\begin{proof}
The first part is proven for \(\sigma_{i}=\sqrt{\lambda_{i}}\).
We shall also prove the last statement.
Compute
\begin{equation}
  \langle T^{\ast}(w_{i}),v_{j}\rangle_{V}=\langle w_{i},T(v_{j})\rangle_{W}
  =\begin{cases}
  \sigma_{i}\delta_{ij},~&\mbox{for}~1\le i\le r\\
  \mathbf{0},~&\mbox{otherwise}.
  \end{cases}
\end{equation}
Hence we have
\begin{equation}
  T^{\ast}(w_{i})=\begin{cases}
  \sigma_{i}v_{i},~&\mbox{for}~1\le i\le r\\
  \mathbf{0},~&\mbox{otherwise}.
  \end{cases}
\end{equation}
We then infer that for \(1\le i\le r\) 
\begin{equation}
  T^{\ast}T(v_{i})=\sigma_{i}^{2}v_{i}
\end{equation}
and \(T^{\ast}T(v_{i})=\mathbf{0}\) for \(i>r\).
\end{proof}

\begin{definition}
The scalars \(\sigma_{1},\ldots,\sigma_{r}\) are called the
\emph{singular values of \(T\)}. If \(r<k:=\min\{m,n\}\), then
the term ``singular value'' is extended to include \(\sigma_{r+1}=\cdots =\sigma_{k}=0\).
\end{definition}

\begin{remark}
From the proof, we see that the singulars of \(T\) and
the singular values of its adjoint are identical. Indeed, we have seen that 
\begin{equation}
  T^{\ast}(w_{i})=\begin{cases}
  \sigma_{i}v_{i},~&\mbox{for}~1\le i\le r\\
  \mathbf{0},~&\mbox{otherwise}.
  \end{cases}
\end{equation}
\end{remark}
